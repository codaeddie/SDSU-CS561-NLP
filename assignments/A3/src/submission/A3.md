# Note. Here are some things to keep in mind as you plan your time for this assignment.

-   The total amount of pytorch code to write, and code complexity, of this assignment is lower than Assignment 4. However, you’re also given less guidance or scaffolding in how to write the code.
-   This assignment involves a pretraining step that takes approximately 2 hours to perform on Azure, and you’ll have to do it twice.

You’ll train a Transformer model to attempt to answer simple questions of the form “Where was person [x] born?” – without providing any input text from which to draw the answer. You’ll find that models are able to learn some facts about where people were born through pretraining, and access that information during fine-tuning to answer the questions. Then, you’ll take a harder look at the system you built, and reason about the implications and concerns about relying on such implicit pretrained knowledge.

### You’ll need around 5 hours for training.

## Pretrained Transformer models and knowledge access

-   1.  [0 points (Coding)] Review the minGPT demo code.
    2.  1.  In the src/submission/mingpt-demo/ folder, there is a Jupyter notebook (play char.ipynb) that trains and samples from a Transformer language model. Take a look at it locally on your computer and you might need to install Jupyter notebootk pip install jupyter to get somewhat familiar with the code how it defines and trains models. You don’t need to run the train locally, because training will take long time on CPU only local environment. Some of the code you are writing below will be inspired by what you see in this notebook.
        2.  Note that you do not have to write any code or submit written answers for this part.
    3.  [0 points (Coding)] Read through NameDataset in src/submission/dataset.py 
    4.  (c) [4 points (Coding)] Implement finetuning (without pretraining).

## Take a look at src/submission/helper.py

```python
from .model import GPT
from .dataset import NameDataset, CharCorruptionDataset
from .trainer import Trainer, TrainerConfig


import torch
import random
random.seed(0)
device = torch.cuda.current_device() if torch.cuda.is_available() else 'cpu'




def initialize_vanilla_model(mconf):
    attention_model = None
    ### TODO:
    ### [part c]: Make some model here


    ### START CODE HERE
    model = GPT(mconf).to(device)
    ### END CODE HERE
    return attention_model


def initialize_synthesizer_model(mconf):
    attention_model = None
    ### TODO
    ### [part g]: Make some other model here


    ### START CODE HERE
    ### END CODE HERE
    return attention_model


def finetune(reading_params_path, finetune_corpus_path, pretrain_dataset, block_size, model):
    ### TODO:
    ### [part c] [part f]:
    ### - Given:
    ###     1. A finetuning corpus specified in finetune_corpus_path
    ###     2. A path reading_params_path containing pretrained model
    ###         parameters, or None if finetuning without a pretrained model
    ###     3. An output path writing_params_path for the model parameters
    ### - Goals:
    ###     1. If reading_params_path is specified, load these parameters
    ###         into the model
    ###     2. Finetune the model on this corpus
    ###
    ### - Make sure to use the following hyperparameters:
    ###     Hyperparameters for finetuning WITHOUT a pretrained model:
    ###         max_epochs=75
    ###         batch_size=256
    ###         learning_rate=6e-4
    ###         lr_decay=True
    ###         warmup_tokens=512*20
    ###         final_tokens=200*len(pretrain_dataset)*block_size
    ###         num_workers=4
    ###     Hyperparameters for finetuning WITH a pretrained model:
    ###         max_epochs=10
    ###         batch_size=256
    ###         learning_rate=6e-4
    ###         lr_decay=True
    ###         warmup_tokens=512*20
    ###         final_tokens=200*len(pretrain_dataset)*block_size
    ###         num_workers=4
    ###
    ###
    ### Note: Please use torch.load(reading_params_path, map_location=torch.device('cpu')) to load pretrained model


    trainer_obj = None #Trainer object (see trainer.py for more details)
    tconf = None #TrainerConfig object (see trainer.py for more details)
    ### START CODE HERE


    ### END CODE HERE
    return tconf, trainer_obj


def pretrain(pretrain_dataset, block_size, model):
    ### TODO:
    ### [part f]:
    ### - Given:
    ###     1. A corpus specified in pretrain_corpus_path
    ### - Goals:
    ###     1. Pretrain the model on this corpus
    ###
    ### - Make sure to use the following hyperparameters for pretraining:
    ###     max_epochs=650
    ###     batch_size=128
    ###     learning_rate=6e-3
    ###     lr_decay=True
    ###     warmup_tokens=512*20
    ###     final_tokens=200*len(pretrain_dataset)*block_size
    ###     num_workers=4


    trainer_obj = None #Trainer object (see trainer.py for more details)
    tconf = None #TrainerConfig object (see trainer.py for more details)


    ### START CODE HERE
    ### END CODE HERE
    return tconf, trainer_obj


def train(model, writing_params_path, trainer_obj):
    ### TODO:
    ### [part c]:
    ###
    ### Note: trainer_obj is of type Trainer (see trainer.py for more details)


    ### START CODE HERE
    ### END CODE HERE
    return
```

## [helper.py](http://helper.py)

Eventually you will pretrain and finetune a model. For now, focus on the finetune without pretraining

Modify [part c]

initialize

![image.png](https://codahosted.io/docs/jpcURNrDyl/blobs/bl-FHTR2NR6C1/d6657bcd08c1380138e26e1bb45341e45a4fa8b8542056d311c0df9ecc1cd6dffdb6a7944c9e28a63e4fe467ccb7813a7f9190f1034823959ea8f5e9d044b03c1590bef53ccb8c2231f8b9e43538c02384cb888b8bee2fa4b7cdc566b6e679479b5206f4)

finetune

![image.png](https://codahosted.io/docs/jpcURNrDyl/blobs/bl-waTXcxa7SV/37bc85ac5fa0657379cfd87f8f99ad36d8e0a8d677c880e01447bbaab4e6a7370803f233e3776da531061be7d31828289b4f2017d28839ebdf61273effdbb7133f665800dbfd653dbb7ff3eeda8075bf7e67e3ab558f2a34128ec7b89d409798a493e83e)

-   -   train

![image.png](https://codahosted.io/docs/jpcURNrDyl/blobs/bl-bUwworH-PX/595a6fd2a9d2e89d317687ced5ce76779025d5a5020024f43aa6dafc51e821ed6d38552b317d64275b3b7e526f632f8e247fd8f54916efc2e7e64a75bd4c4559c9b162bf4259f695a68b4d9e814d3699d965609c763e7d60a39077bfd01662ec1ce2470b)

1.  Check if the reading_params_path is provided. If it is, load the pretrained model parameters into the model using torch.load() Since we are not pretraining in this case, this step can be skipped.
2.  Initialize the NameDataset for finetuning using the finetune_corpus_path.
3.  Set the hyperparameters for the TrainerConfig.
4.  Initialize the Trainer with the model, finetune dataset, and trainer configuration.
5.  Train the model using the Trainer.

trainer_obj.train() in def finetune(): 

This line is responsible for training the model on the finetuning dataset. The Trainer object is initialized with the model, the finetuning dataset, and the training configuration. The train() method of the Trainer object is then called to start the training process.  

-   trainer_obj.train() in def train(): 

This line is also responsible for training the model. However, this function is more general and can be used for both pretraining and finetuning. The Trainer object is passed as an argument to this function, and the train() method is called to start the training process.  

The train() method in both functions is essentially doing the same thing - training the model. The difference lies in the context in which they are used. In finetune(), it's specifically for finetuning the model, while in train(), it's a more general function that can be used for both pretraining and finetuning.

------

# NO - Pretrain RESULTS

------

After running ./run.sh vanilla_finetune_without_pretrain

![image.png](https://codahosted.io/docs/jpcURNrDyl/blobs/bl-d7-GPMN2t0/c834cc04a26b5c2ad4d479c3dd61776c1e48ab4785580b1127ea1c50400b466fd03f8bb0bb2eec4c82859611e829b14d885f34570bb51aed779ca15565ff0b67fd4c5bb0aba8809f88c8f534c625627b648ee46babb2e2648fd2d927a180363ea8cb9bb5)

------

After running ./run.sh vanilla_eval_dev_without_pretrain

![image.png](https://codahosted.io/docs/jpcURNrDyl/blobs/bl-CQciCnl1LN/6f465f76747efb4846e4fa7c0a26a31ad1d848f5fa7e096a285086764b7899578794adfb2f1411f924892b8a008dc4905e4cde1e16be8007c790207b8feb4d6b7f5dba3dd14824a18ecf8a5675d6bb5dbcd94d38678cb17e88355c9619562abb52bb568b)

![image.png](https://codahosted.io/docs/jpcURNrDyl/blobs/bl-H67-8qjAwr/322e5ea1320af7ebd1e15539f226cab23503e87b13a744d34d7f38525d3f43b3e209b3c75505ac9092fc98ae901a7bfac789ba5c4a1c27e194eb23825bcba5d88206cf6f4c456e737fe5a7b8c0a6a03e7490f88f075d763e2759349f72c9438b302852e5)

------

After running ./run.sh vanilla_eval_test_without_pretrain

![image.png](https://codahosted.io/docs/jpcURNrDyl/blobs/bl-PkmJX5lMOm/83ad84f1a3e2dc0dddc50e097c99a545b94853808f7def6925dc69369dd59c238ecf237931ee6ab42e356af21c6b41d7aea609cf718f47f3645f832165da6d7af502fa1bbec8c15a24f1b41fad5e768ffa15041114035bd65f9becbbbfa67f40d6bf42cc)

![image.png](https://codahosted.io/docs/jpcURNrDyl/blobs/bl-t38o1yGa5Z/3b79b722d4b42c48ef00be568f59e28c6ffa0340ed567169218277f25e7f92eaed8d712eb2ef521c15053c0b6427cbc8ddef74dadbaf204c13a8557c1cbb037e80736d97e2aa38d176ca3de4fcd61b68c48371a60f8c22aa08de1a6c96aa0aee95bc0be1)

------

# Pretraining 

1.  In the file src/submission/dataset.py, implement the getitem() function for the dataset class CharCorruptionDataset 
2.  -   Follow the instructions provided in the comments in dataset.py  Span corruption is explored in the T5 paper [2]. It randomly selects spans of text in a document and replaces them with unique tokens (noising). Models take this noised text, and are required to output a pattern of each unique sentinel followed by the tokens that were replaced by that sentinel in the input. In this question, you’ll implement a simplification that only masks out a single sequence of characters. 
    -   This question will be graded via autograder based on your whether span corruption function implements some basic properties of our spec. We’ll instantiate the CharCorruptionDataset with our own data, and draw examples from it.
3.  Pretrain, finetune, and make predictions 
4.  -   Now fill in the pretrain portion of src/submission/helper.py, which will pretrain a model on the span corruption task.
    -   Additionally, modify your finetune portion to handle fine-tuning in the case with pretraining.
    -   In particular, if a path to a pretrained model is provided in the bash command, load this model before finetuning it on the birth-place prediction task. 
    -   Pretrain your model on wiki.txt (which should take approximately two hours), finetune it on NameDataset and evaluate it. Specifically, you should be able to run the following four commands:

```
# Pretrain the model
./run.sh vanilla_pretrain


# Finetune the model
./run.sh vanilla_finetune_with_pretrain


# Evaluate the model - write to disk
./run.sh vanilla_eval_dev_with_pretrain


# Evaluate the model on the test set - write to disk
./run.sh vanilla_eval_test_with_pretrain
```

------

------

# Synthesizer

### Research! Write and try out the synthesizer variant (Budget 2 hours for pretraining!) 

We'll now go to changing the Transformer architecture itself – specifically, the self-attention module. While we’ve been using a self-attention scoring function based on dot products, this involves a rather intensive computation that’s quadratic in the sequence length. 

-   This is because the dot product between 𝓁 ² pairs of word vectors is computed in each computation. Synthesized attention is a very recent alternative that has potential benefits by removing this dot product (and quadratic computation) entirely.

It’s a promising idea, and one way for us to ask, 

>   What's important/right about the Transformer architecture, and where can we improve/prune aspects of it?

-   In attentton.py implement the forward method of SynthesizerAttention, which implements a variant of the Synthesizer proposed in the cited paper.

![image.png](https://codahosted.io/docs/jpcURNrDyl/blobs/bl-1gv9XrKxbF/232423bdf9fef9ea0c076adf757a6561bb7c094a6a1c227482219ef2dc4ed4d621f082ee45f4c6aa42cd1bf0c45298f5f70f3ac7000222be38c76c2834d1c789d44958d14dbc992f00b0b435addd427f920ebb3efbb0b5812e44572e93f39cbf8bf53f59)In the rest of the code in the src/submission/helper.py folder:

-   modify your model to support using either `CausalSelfAttention` or `SynthesizerAttention`. 
-   Add the ability to switch between these attention variants depending on whether “vanilla” (for causal self-attention) or “synthesizer” (for the synthesizer variant) is selected in the command line arguments.
-   (see the section marked [part g] in src/submission/helper.py). 

Below are bash commands that your code should support in order to pretrain the model, finetune it, and make predictions on the dev and test sets. 

```
# Pretrain the model
./run.sh synthesizer_pretrain


# Finetune the model
./run.sh synthesizer_finetune_with_pretrain


# Evaluate on the dev set; write to disk
./run.sh synthesizer_eval_dev_with_pretrain


# Evaluate on the test set; write to disk
./run.sh synthesizer_eval_test_with_pretrain
```

**Note that the pretraining process will take approximately 2 hours.**

------

# Deliverables

Within the src/submission directory:

-   src/submission/ init .py
-   src/submission/attention.py
-   src/submission/dataset.py
-   src/submission/helper.py
-   src/submission/model.py
-   src/submission/trainer.py
-   src/submission/utils.py
-   src/submission/vanilla.model.params
-   src/submission/vanilla.nopretrain.dev.predictions
-   src/submission/vanilla.nopretrain.test.predictions
-   src/submission/vanilla.pretrain.params
-   src/submission/vanilla.finetune.params
-   src/submission/vanilla.pretrain.dev.predictions
-   src/submission/vanilla.pretrain.test.predictions
-   src/submission/synthesizer.pretrain.params
-   src/submission/synthesizer.finetune.params
-   src/submission/synthesizer.pretrain.dev.predictions
-   src/submission/synthesizer.pretrain.test.predictions

------

# Coding

![image.png](https://codahosted.io/docs/jpcURNrDyl/blobs/bl-bgBBSA3bps/6faa57c64aeecf1726435d5947fbe5e5f9f5f47b54f5772277607d4da592095f2e68cf1a0d7d92d25011b3a632338cdcfe46e873697756388a5f9ea828f70bb4a293ee4471c2c7c5cc1386249420550d93fb3d6670beb21dbf4e6f5df201842e9ca760b1)

![image.png](https://codahosted.io/docs/jpcURNrDyl/blobs/bl-DLbBHJ9vRr/274500599367dd4b8f1aca7a4c2c31feeea49f466e41ceb5d256a1b2fc4be6749e96ecaef93c9a4b35059ba7721348dd1c8fa7d3a0482363d032a271cfe455a6aa3fb490b2d156fff1b6004dd9a560983442e17900736fcc197ca7921c46266161e75ebf)

### **Create a copy of** **A3** **directory and name it** **A301** **in your VM**:

-   -   First, navigate to the parent directory of **A3** on your VM:

```
cd ~/notebooks 
```

-   -   Then, copy the **A3** directory and name it **A301**:

```
cp -r A3 A301 
```

1.  **Compress the** **A301** **directory**:
2.  -   This step will make the transfer faster and easier to handle. In the VM, run:

```
tar -czvf A301.tar.gz A301 
```

-   -   This command creates a compressed archive **A301.tar.gz** of the **A301** directory.

1.  **Transfer** **A301.tar.gz** **from VM to your local machine**:
2.  -   On your local machine, run the following **scp** command to transfer the compressed file:

```
scp -P 65293 scpdxcs@ml-lab-5f0456f1-7704-4e40-bc2a-912b128b8483.southcentralus.cloudapp.azure.com:~/notebooks/A301.tar.gz /home/eddie/workplace/cs561/ 
```

-   -   This command uses the **scp** (secure copy) protocol to transfer the file. Replace **65293** with your VM's SSH port number, if different. The command specifies the source (your VM) and the destination (your local machine's directory).

1.  **Decompress the** **A301.tar.gz** **file on your local machine**:
2.  -   Once the transfer is complete, you can decompress the archive to get the **A301** directory on your local machine. Run:

```
tar -xzvf A301.tar.gz 
```

-   -   This will extract the **A301** directory in your current local directory (**/home/eddie/workplace/cs561/**).

```
def create_model(args, mconf):
    if args['--variant'] == 'vanilla':
        return initialize_vanilla_model(mconf)
    else:
        return initialize_synthesizer_model(mconf)


# Changes to 


def create_model(args, mconf):
    variant = args['--variant']
    if variant == 'vanilla':
        return initialize_vanilla_model(mconf)
    elif variant == 'synthesizer':
        return initialize_synthesizer_model(mconf)
    else:
        raise ValueError(f"Invalid variant: {variant}. Expected 'vanilla' or 'synthesizer'.")
```

Xthequickbrownfox321

```
eddie@eddie-pcmasterrace:~/workplace/cs561/A3/src$ tree
.
├── data
│   ├── birth_dev.tsv
│   ├── birth_places_train.tsv
│   ├── birth_test_inputs.tsv
│   ├── birth_test.tsv
│   └── wiki.txt
├── environment_gpu.yml
├── environment.yml
├── grader.py
├── graderUtil.py
├── __pycache__
│   ├── grader.cpython-37.pyc
│   ├── graderUtil.cpython-37.pyc
│   └── graderUtil.cpython-38.pyc
├── run.py
├── run.sh
└── submission
    ├── attention.py
    ├── dataset.py
    ├── helper.py
    ├── __init__.py
    ├── model.py
    ├── trainer.py
    └── utils.py


scpdxcs@ML-RefVm-80198:~/notebooks/A3/src$ conda activate CS561_GPU
(CS561_GPU) scpdxcs@ML-RefVm-80198:~/notebooks/A3/src$ nvidia-smi
Failed to initialize NVML: Driver/library version mismatch
(CS561_GPU) scpdxcs@ML-RefVm-80198:~/notebooks/A3/src$ python
Python 3.8.10 (default, Jun  4 2021, 15:09:15) 
[GCC 7.5.0] :: Anaconda, Inc. on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> import torch
>>> torch.cuda.current_device()
0
>>> torch.cuda.device(0)
<torch.cuda.device object at 0x7f8aa9a0df10>
>>> torch.cuda.device_count()
1
>>> torch.cuda.get_device_name()
'Tesla K80'
>>> exit
Use exit() or Ctrl-D (i.e. EOF) to exit
>>> exit()
(CS561_GPU) scpdxcs@ML-RefVm-80198:~/notebooks/A3/src$ ls
__pycache__  environment.yml      grader.py      run.py  submission
data         environment_gpu.yml  graderUtil.py  run.sh
(CS561_GPU) scpdxcs@ML-RefVm-80198:~/notebooks/A3/src$ cd 
```

------

# Tmux

![image.png](https://codahosted.io/docs/jpcURNrDyl/blobs/bl-Dk4DQQPVFT/afb6fa92a57de5fd1347ad7683afd53887f694c7ffd982d3dc2867bcee03ef5dccaf9e21ba2ab62d122bb4238a2f3547c3cd96f9ec65b71f06a4f06f28d8ab677d164f5c047bfb627e23bb371ce38ae75922d495d2e3eb7b563649fcf7f831a4386bbeff)

You can detach from your tmux session by pressing **Ctrl+B** then **D**. Tmux operates using a series of keybindings (keyboard shortcuts) triggered by pressing the "prefix" combination. By default, the prefix is **Ctrl+B**. After that, press **D** to detach from the current session.

```
[detached (from session 0)]
```

You're no longer attached to the session, but your long-running command executes safely in the background. You can list active tmux sessions with tmux ls:

```
$ tmux ls


0: 1 windows (created Sat Aug 27 20:54:58 2022)
```

**[ Learn how to** **[manage your Linux environment for success](https://www.redhat.com/en/engage/linux-management-ebook-s-201912231121?intcmp=701f20000012ngPAAQ)****. ]**

You can disconnect your SSH connection at this point, and the command will continue to run. When you're ready, reconnect to the server and reattach to the existing tmux session to resume where you left off:

```
$ tmux attach -t 0
```

Tmux provides several keybindings to execute commands quickly in a tmux session. Here are some of the most useful ones.

First, create a new tmux session if you're not already in one. You can name your session by passing the parameter -s {name} to the tmux new command when creating a new session:

```
$ tmux new -s [yocoda]
```

-   **Ctrl+B D** — Detach from the current session.
-   **Ctrl+B %** — Split the window into two panes horizontally.
-   **Ctrl+B "** — Split the window into two panes vertically.
-   **Ctrl+B Arrow Key** (Left, Right, Up, Down) — Move between panes.
-   **Ctrl+B X** — Close pane.
-   **Ctrl+B C** — Create a new window.
-   **Ctrl+B N** or **P** — Move to the next or previous window.
-   **Ctrl+B 0 (1,2...)** — Move to a specific window by number.
-   **Ctrl+B :** — Enter the command line to type commands. Tab completion is available.
-   **Ctrl+B ?** — View all keybindings. Press **Q** to exit.
-   **Ctrl+B W** — Open a panel to navigate across windows in multiple sessions.

For additional keybindings, consult the tmux man pages.

#  