# CS561 Deep Learning with NLP San Diego State University
**December 2023**

![](https://github.com/codaeddie/SDSU-CS561-NLP/blob/main/assignments/a1/imgs/SDSUu.png)
![](https://github.com/codaeddie/SDSU-CS561-NLP/blob/main/assignments/a1/imgs/SDSUflag.png)

This repository is my complete implementation for assignments and projects in CS561: Deep Learning with Natural Language Processing.

## Assignments
-   Assignment 1: [code](https://github.com/codaeddie/SDSU-CS561-NLP/tree/main/assignments/a1) | [handout](https://github.com/codaeddie/SDSU-CS561-NLP/blob/main/assignments/handouts/A1.pdf) Exploring word vectors
-   Assignment 2: [code](https://github.com/codaeddie/SDSU-CS561-NLP/tree/main/assignments/a2) | [handout](https://github.com/codaeddie/SDSU-CS561-NLP/blob/main/assignments/handouts/A2.pdf) - Training word2vec
-   Assignment 3: [code](https://github.com/codaeddie/SDSU-CS561-NLP/tree/main/assignments/a3) | [handout](https://github.com/codaeddie/SDSU-CS561-NLP/blob/main/assignments/handouts/A3.pdff) - Neural Dependency Parser

**Note, all instructions for CS561 SDSU are detailed in corresponding handout PDF's.**

These assignments are heavily based on the Stanford CS224u course. 

## Final Project

Multitask BERT: 

-   [handout](https://web.stanford.edu/class/cs224n/project/default-final-project-bert-handout.pdf) 
-   [starter code](https://github.com/gpoesia/minbert-default-final-project?tab=readme-ov-file)

---

### Misc. Links:

**Guides:**

-   [Python Tutorial](https://colab.research.google.com/drive/1eO8eLGXxSo9fYMXw-dUEWPwkolIzDpxf?usp=sharing)
-   [Introduction to Jupyter Notebooks](https://github.com/cgpotts/cs224u/blob/main/tutorial_jupyter_notebooks.ipynb)
-   [Getting Started with NumPy](https://github.com/cgpotts/cs224u/blob/main/tutorial_numpy.ipynb)
-   [Getting Started with Pytorch](https://github.com/cgpotts/cs224u/blob/main/tutorial_pytorch.ipynb)

**Scientific Papers:**

-   [Distributed Representations of Words and Phrases](https://proceedings.neurips.cc/paper_files/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)(Negative Sampling)
-   [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)
-   [Layer Normalization](https://arxiv.org/pdf/1607.06450.pdf)
-   [Image Transformer](https://arxiv.org/pdf/1802.05751.pdf)
-   [Music Transformer: Generating music with long-term structure](https://arxiv.org/pdf/1809.04281.pdf)
-   [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)
-   [Contextual Word Representations: A Contextual Introduction](https://arxiv.org/pdf/1902.06006.pdf)

**Readings:**

-   [Understanding Backpropagation](https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b) by Andrej Karpathy
-   [The Illustrated BERT, ELMo, and co.](http://jalammar.github.io/illustrated-bert/)
-   [Martin & Jurafsky Chapter on Transfer Learning](https://web.stanford.edu/~jurafsky/slp3/11.pdf)
-   [Transformer (Google AI blog post)](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)
-   [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
